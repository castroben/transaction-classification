{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f06918-572b-43d5-a08e-905e981a8af9",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "production-ready version saved in corresponding python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617e7c7-929e-42bb-8c24-76f81d9132a7",
   "metadata": {},
   "source": [
    "## Environment Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6b1d1-d6d2-4de2-af89-e5e6818616db",
   "metadata": {},
   "source": [
    "##### Libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dedb649-d272-45b4-8fe9-129d62223db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StringIndexerModel, OneHotEncoder\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bc485d-ebd4-49e6-8c0f-bad8999e88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1adcd-5bd3-4448-a219-07b0ca03b1f5",
   "metadata": {},
   "source": [
    "##### Spark Session Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d24af4f-4848-4831-aa07-ecd2e4d295cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/10 05:40:39 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/02/10 05:40:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/osbdet/.ivy2/cache\n",
      "The jars for the packages stored in: /home/osbdet/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-edfa3e3b-c645-47e1-94bb-338d288bc1d0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 9094ms :: artifacts dl 220ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-edfa3e3b-c645-47e1-94bb-338d288bc1d0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/137ms)\n",
      "25/02/10 05:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 05:42:02 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/02/10 05:42:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cluster relies on Spark '3.5.0'\n"
     ]
    }
   ],
   "source": [
    "spark_session = \\\n",
    "  SparkSession.builder\\\n",
    "            .appName(\"real-time-analytics\")\\\n",
    "            .config(\"spark.sql.caseSensitive\", \"true\")\\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .getOrCreate()\n",
    "spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"This cluster relies on Spark '{spark_session.version}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d29599-72d9-4eea-b676-b3773b8a6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"s3access\")\n",
    "spark_session.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"_s3access123$\")\n",
    "spark_session.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark_session.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark_session.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3e1fd-cd31-4fc1-979a-b402fa9dffbb",
   "metadata": {},
   "source": [
    "## Application Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9194527d-6350-445f-96cd-1752c95dd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic_input = \"transactions_raw_v1\" # topic name for input transaction events\n",
    "kafka_topic_output = \"transactions_classified_v1\" # topic name input transaction classification events\n",
    "kafka_bootstrap_server_address = \"localhost:9092\" # server address for kafka connection\n",
    "kafka_streaming_checkpoint_filepath = \"/home/osbdet/notebooks/real-time-analytics/streaming_checkpoint\"\n",
    "\n",
    "\n",
    "s3_files_timestamp_col = \"trans_date_trans_time\" # timestamp col name of raw events landed in object storage\n",
    "num_historical_transactions = 3 # transaction history threshold for processing\n",
    "string_indexer_filepath = \"/home/osbdet/notebooks/real-time-analytics/classifier/string_indexer\"\n",
    "rf_classifier_filepath = \"/home/osbdet/notebooks/real-time-analytics/classifier/model\" # filepath where classifier model is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b9865-2aae-4cb0-b58d-0ca299ae7685",
   "metadata": {},
   "source": [
    "##### Schema Config\n",
    "\n",
    "The below configurations are needed given lack of a schema registry\n",
    "\n",
    "* schema_ddl is Python string literal with schema in DDL format\n",
    "* schema_ddl must match avro schema defined when serializing .csv records\n",
    "\n",
    "avro schema (see ingestion_engine.xml for further details):\n",
    "\n",
    "`{`<br>\n",
    "`  \"name\": \"Transaction\",`<br>\n",
    "`  \"namespace\": \"com.example\",`<br>\n",
    "`  \"type\": \"record\",`<br>\n",
    "`  \"fields\": [`<br>\n",
    "`      {\"name\": \"cc_num\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"trans_num\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"trans_date_trans_time\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"merchant\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"category\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"amt\", \"type\": \"double\"},`<br>\n",
    "`      {\"name\": \"first\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"last\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"gender\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"street\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"city\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"state\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"zip\", \"type\": \"int\"},`<br>\n",
    "`      {\"name\": \"lat\", \"type\": \"double\"},`<br>\n",
    "`      {\"name\": \"long\", \"type\": \"double\"},`<br>\n",
    "`      {\"name\": \"city_pop\", \"type\": \"int\"},`<br>\n",
    "`      {\"name\": \"job\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"dob\", \"type\": \"string\"},`<br>\n",
    "`      {\"name\": \"unix_time\", \"type\": \"long\"},`<br>\n",
    "`      {\"name\": \"merch_lat\", \"type\": \"double\"},`<br>\n",
    "`      {\"name\": \"merch_long\", \"type\": \"double\"}`<br>\n",
    "`  ]` <br>\n",
    "`}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1367f2-3249-44e8-bf57-29b0dee93f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "        cc_num BIGINT,\n",
    "        trans_num STRING,\n",
    "        trans_date_trans_time STRING,\n",
    "        merchant STRING,\n",
    "        category STRING,\n",
    "        amt DOUBLE,\n",
    "        first STRING,\n",
    "        last STRING,\n",
    "        gender STRING,\n",
    "        street STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        zip INT,\n",
    "        lat DOUBLE,\n",
    "        long DOUBLE,\n",
    "        city_pop INT,\n",
    "        job STRING,\n",
    "        dob STRING,\n",
    "        unix_time BIGINT,\n",
    "        merch_lat DOUBLE,\n",
    "        merch_long DOUBLE\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c8923-d749-485b-906e-efbdaac5f6c5",
   "metadata": {},
   "source": [
    "## Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4cef5e-b3d1-4d88-8a68-90f12d76fe91",
   "metadata": {},
   "source": [
    "### Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e254fa4e-a1d9-4bf3-b381-cbaa2c9d4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_history(cc_num, num_historical_transactions):\n",
    "    path = f\"s3a://raw-transactions/{cc_num}\"\n",
    "    relevant_columns = [\"cc_num\", \"trans_date_trans_time\", \"amt\", \"lat\", \"long\", \"merch_lat\", \"merch_long\", \"category\"]\n",
    "\n",
    "    try:\n",
    "        # Read the Parquet files and add file_name column\n",
    "        data = spark_session.read.parquet(path).withColumn(\"file_name\", f.input_file_name())\n",
    "\n",
    "        # Count unique files\n",
    "        file_count = data.select(\"file_name\").distinct().count()\n",
    "\n",
    "        # Ensure enough files exist\n",
    "        if file_count <= num_historical_transactions:\n",
    "            print(\"not enough history\")\n",
    "            return None\n",
    "\n",
    "        # Select the top transactions (assuming a timestamp column exists)\n",
    "        if \"transaction_time\" in data.columns:\n",
    "            data = data.orderBy(\"transaction_time\", ascending=False).limit(num_historical_transactions)\n",
    "        else:\n",
    "            data = data.limit(num_historical_transactions)\n",
    "\n",
    "        return data\\\n",
    "                .withColumn(\"trans_date_trans_time\", f.to_timestamp(\"trans_date_trans_time\", \"dd/MM/yyyy HH:mm\"))\\\n",
    "                .select(relevant_columns)\n",
    "                \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab540d0e-aa1e-40cb-a7f9-c74d311b3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transaction(df_transaction):\n",
    "    df_transaction_clean = df_transaction\\\n",
    "            .withColumn(\"trans_date_trans_time\", f.to_timestamp(\"trans_date_trans_time\", \"dd/MM/yyyy HH:mm\"))\\\n",
    "            .withColumn(\"dob\", f.to_date(f.col(\"dob\"),\"dd/MM/yyyy\"))\\\n",
    "            .withColumn(\"hour\", f.hour(\"trans_date_trans_time\"))\\\n",
    "            .withColumn(\"day\", f.dayofweek(\"trans_date_trans_time\"))\\\n",
    "            .withColumn(\"month\", f.month(\"trans_date_trans_time\"))\\\n",
    "            .withColumn(\"year\", f.year(\"trans_date_trans_time\"))\\\n",
    "            .withColumn(\"weekend\", f.when(f.col(\"day\").isin(1, 7), 1).otherwise(0))\\\n",
    "            .withColumn(\"customer_age\", f.round(f.datediff(f.col(\"trans_date_trans_time\"), f.col(\"dob\"))/365,0).cast('Integer'))\\\n",
    "            .withColumn(\"distance\", f.sqrt(f.pow(f.col(\"lat\") - f.col(\"merch_lat\"), 2) + f.pow(f.col(\"long\") - f.col(\"merch_long\"), 2)))\\\n",
    "            .withColumn(\"gender\", f.when(f.col(\"gender\") == \"Male\", 1).otherwise(0))\\\n",
    "            .drop(*[\"merchant\", \"first\", \"last\", \"job\", \"dob\", \"street\", \"city\", \"state\", \"zip\", \"city_pop\"])\n",
    "    \n",
    "    return df_transaction_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b3e753-eedd-4fb6-b827-757f60b94bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_transaction(df_transaction, df_transaction_history):\n",
    "    window = Window.partitionBy(\"cc_num\").orderBy(f.col(\"trans_date_trans_time\"))\n",
    "\n",
    "    # Amount Statistics\n",
    "    df_amt_stats = df_transaction_history\\\n",
    "            .agg(\n",
    "                f.mean(f.col(\"amt\")).alias(\"historical_mean_amt\"),\n",
    "                f.stddev(f.col(\"amt\")).alias(\"historical_std_amt\"),\n",
    "                f.max(f.col(\"amt\")).alias(\"historical_max_amt\"),\n",
    "                f.min(f.col(\"amt\")).alias(\"historical_min_amt\")\n",
    "            )\n",
    "\n",
    "    # Average Time Diff b/w Transactions\n",
    "    df_time_diff = df_transaction_history.withColumn(\"prev_trans_time\", f.lag(\"trans_date_trans_time\").over(window))\n",
    "    df_avg_time_diff = df_time_diff.agg(f.mean(f.col(\"prev_trans_time\")).alias(\"historical_avg_time_diff\"))\n",
    "    \n",
    "    # Distance Statistics\n",
    "    df_distance = df_transaction_history.withColumn(\n",
    "        \"distance\", f.sqrt((f.col(\"lat\") - f.col(\"merch_lat\"))**2 + (f.col(\"long\") - f.col(\"merch_long\"))**2)\n",
    "    )\n",
    "    df_avg_distance = df_distance\\\n",
    "                        .agg(\n",
    "                            f.mean(f.col(\"distance\")).alias(\"historical_avg_distance_from_merchant\"),\n",
    "                            f.stddev(f.col(\"distance\")).alias(\"historical_std_distance_from_merchant\")\n",
    "                        )\n",
    "    \n",
    "    # Number of distinct categories\n",
    "    df_num_categories = df_transaction_history.agg(f.countDistinct(\"category\").cast(\"int\").alias(\"historical_num_categories\"))\n",
    "\n",
    "    # Join Historical Statistics\n",
    "    df_transaction_enriched = df_transaction.crossJoin(df_amt_stats).crossJoin(df_avg_time_diff).crossJoin(df_avg_distance).crossJoin(df_num_categories)\n",
    "\n",
    "    return df_transaction_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0013d5ce-d8db-4065-8d7b-5e3df1de020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_transaction(df_transaction):\n",
    "    # Load the indexer model - inputCol=\"category\", outputCol=\"category_index\"\n",
    "    indexer = StringIndexerModel.load(string_indexer_filepath)\n",
    "    \n",
    "    # Convert categorical column to numeric index using StringIndexer\n",
    "    df_indexed = indexer.transform(df_transaction)\n",
    "\n",
    "    # Encode categorical column\n",
    "    encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_encoded\")\n",
    "    df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    # df_encoded = df_encoded.drop(*[\"category\", \"category_index\", \"trans_date_trans_time\"])\n",
    "    df_encoded = df_encoded.drop(*[\"category_index\"])\n",
    "\n",
    "    return df_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec45032e-0544-463d-bd44-90526f192285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_transaction(df_transaction):\n",
    "    # Define Vector Assembler\n",
    "    # feature_columns = [col for col in df_transaction.columns if col != \"trans_num\"]\n",
    "    feature_columns = [col for col in df_transaction.columns if col not in {\"trans_num\", \"trans_date_trans_time\", \"category\"}]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # Vectorize features\n",
    "    df_vectorized = assembler.transform(df_transaction)\n",
    "\n",
    "    return df_vectorized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebe6715-e04f-4a0c-aaf2-7d31d4640a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_classification_event(df_transaction_classified):\n",
    "    df_classification_event = df_transaction_classified.drop(\"category_encoded\", \"features\", \"rawPrediction\", \"probability\")\n",
    "    df_classification_event_json = df_transaction_classified.selectExpr(\"trans_num AS key\", \"to_json(struct(*)) AS value\")\n",
    "\n",
    "    return df_classification_event_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ae597e-09c1-41c6-80b7-d28393828e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transaction(df_transaction, batch_id):\n",
    "    if df_transaction.isEmpty():\n",
    "        print(f\"Skipping batch {batch_id} as it contains no data\")\n",
    "        return\n",
    "\n",
    "    print(\"raw transaction:\\n\")\n",
    "    df_transaction.printSchema()\n",
    "    \n",
    "    cc_num = df_transaction.select(\"cc_num\").first()[0]\n",
    "    trans_num = df_transaction.select(\"trans_num\").first()[0]\n",
    "\n",
    "    print(f\"Processing trans_num={trans_num} for cc_num={cc_num}\")\n",
    "\n",
    "    # retrieve transaction history\n",
    "    df_transaction_history = get_transaction_history(cc_num, num_historical_transactions)\n",
    "    if df_transaction_history is None:\n",
    "        print(f\"skipping over trans_num={trans_num} for cc_num={cc_num} due to lack of history\")\n",
    "        return\n",
    "\n",
    "    # clean raw transaction data\n",
    "    df_transaction_clean = clean_transaction(df_transaction)\n",
    "    print(\"clean transaction:\\n\")\n",
    "    df_transaction_clean.printSchema()\n",
    "\n",
    "    # enrich with historical statistics\n",
    "    df_transaction_enriched = enrich_transaction(df_transaction_clean, df_transaction_history)\n",
    "    print(\"enriched transaction:\\n\")\n",
    "    df_transaction_enriched.printSchema()\n",
    "\n",
    "    # encode transaction\n",
    "    df_transaction_encoded = encode_transaction(df_transaction_enriched)\n",
    "    print(\"encoded transaction:\\n\")\n",
    "    df_transaction_encoded.printSchema()\n",
    "\n",
    "    # vectorize transaction\n",
    "    df_transaction_vectorized = vectorize_transaction(df_transaction_encoded)\n",
    "    print(\"vectorized transaction:\\n\")\n",
    "    df_transaction_vectorized.printSchema()\n",
    "\n",
    "    # load classifier\n",
    "    loaded_rf_model = RandomForestClassificationModel.load(rf_classifier_filepath)\n",
    "\n",
    "    # classify transaction\n",
    "    df_transaction_classified = loaded_rf_model.transform(df_transaction_vectorized)\n",
    "    print(\"Number of features in trained model: \", loaded_rf_model.numFeatures)\n",
    "    print(\"Number of features in vectorized data: \", df_transaction_vectorized.select(\"features\").first()[0].size)\n",
    "\n",
    "    print(\"classified transaction:\\n\")\n",
    "    df_transaction_classified.printSchema()\n",
    "\n",
    "    # construct classification event\n",
    "    df_transaction_classification_event = construct_classification_event(df_transaction_classified)\n",
    "    \n",
    "\n",
    "    # Write JSON to Kafka in append mode\n",
    "    df_transaction_classification_event.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_server_address) \\\n",
    "        .option(\"topic\", kafka_topic_output) \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "        .save()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758daf7-9a16-45f8-8190-62e820345d26",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22bbeeea-c1ec-4f62-aada-90dd0e8a8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction_raw = \\\n",
    "  spark_session.readStream\\\n",
    "               .format(\"kafka\")\\\n",
    "               .option(\"kafka.bootstrap.servers\", kafka_bootstrap_server_address)\\\n",
    "               .option(\"subscribe\", kafka_topic_input)\\\n",
    "               .option(\"startingOffsets\", \"latest\")\\\n",
    "               .load()\n",
    "\n",
    "df_transaction_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95a0df52-8751-44f2-8379-13e81c948a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction_deserialized = df_transaction_raw.selectExpr(\"CAST(value AS STRING) AS transaction\") \\\n",
    "        .select(f.from_csv(f.col(\"transaction\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "df_transaction_deserialized.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b556a6d-9a9f-4ea6-b029-44af917e3c45",
   "metadata": {},
   "source": [
    "### Data Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8f59285-5dc0-42af-b72d-e9553c50b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trans_num=61dca41a9728ea5fd6db99efd59768f8 for cc_num=6010000000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = false)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      "\n",
      "enriched transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = false)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- historical_mean_amt: double (nullable = true)\n",
      " |-- historical_std_amt: double (nullable = true)\n",
      " |-- historical_max_amt: double (nullable = true)\n",
      " |-- historical_min_amt: double (nullable = true)\n",
      " |-- historical_avg_time_diff: double (nullable = true)\n",
      " |-- historical_avg_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_std_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_num_categories: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = false)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- historical_mean_amt: double (nullable = true)\n",
      " |-- historical_std_amt: double (nullable = true)\n",
      " |-- historical_max_amt: double (nullable = true)\n",
      " |-- historical_min_amt: double (nullable = true)\n",
      " |-- historical_avg_time_diff: double (nullable = true)\n",
      " |-- historical_avg_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_std_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_num_categories: integer (nullable = false)\n",
      " |-- category_encoded: vector (nullable = true)\n",
      "\n",
      "vectorized transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = false)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- historical_mean_amt: double (nullable = true)\n",
      " |-- historical_std_amt: double (nullable = true)\n",
      " |-- historical_max_amt: double (nullable = true)\n",
      " |-- historical_min_amt: double (nullable = true)\n",
      " |-- historical_avg_time_diff: double (nullable = true)\n",
      " |-- historical_avg_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_std_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_num_categories: integer (nullable = false)\n",
      " |-- category_encoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in trained model:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in vectorized data:  36\n",
      "classified transaction:\n",
      "\n",
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: integer (nullable = false)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- unix_time: long (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- historical_mean_amt: double (nullable = true)\n",
      " |-- historical_std_amt: double (nullable = true)\n",
      " |-- historical_max_amt: double (nullable = true)\n",
      " |-- historical_min_amt: double (nullable = true)\n",
      " |-- historical_avg_time_diff: double (nullable = true)\n",
      " |-- historical_avg_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_std_distance_from_merchant: double (nullable = true)\n",
      " |-- historical_num_categories: integer (nullable = false)\n",
      " |-- category_encoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sink = df_transaction_deserialized \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_transaction) \\\n",
    "    .option(\"checkpointLocation\", \"/home/osbdet/notebooks/real-time-analytics/streaming_checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ce789ec-fea0-466a-8bda-41c2afdfe528",
   "metadata": {},
   "outputs": [],
   "source": [
    "sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a3810-6670-420e-9dc3-6c188480f7ce",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85127180-82ab-41ea-8dee-4a348961943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"cc_num\", LongType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"trans_date_trans_time\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", IntegerType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"unix_time\", LongType(), True),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data_sample = [\n",
    "    (6010000000000000, \"txn0001\", \"07/02/2020 10:00:00\", \"merchant1\", \"category1\", 100.0, \"Alice\", \"Smith\", \"F\", \"123 Main St\", \"Austin\", \"TX\", 78701, 30.2672, -97.7431, 1000000, \"Engineer\", \"01/01/1990\", 1615158000, 30.2672, -97.7431)\n",
    "]\n",
    "\n",
    "df_sample = spark_session.createDataFrame(data_sample, schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
